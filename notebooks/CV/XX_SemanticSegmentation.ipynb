{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семантическая Сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семантическая сегментация - это процесс, когда каждый пиксель исходного изображения связывается с меткой класса, то есть для каждого пикселя изображения мы можем сказать, какой класс он показывает. Классом может быть всё, что угодно, например: кошка, цветок, лев, человек и т.д. \n",
    "\n",
    "Семантическую сегментацию можно рассматривать как классификацию изображений на уровне пикелей. Следовательно, при семантической сегментации каждый пиксель изображения должен быть связан с определённой меткой класса. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/cv_19.jpeg?raw=true\" alt=\"drawing\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation vs. Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим пример, где у нас есть картинка с 6-ю людьми на ней. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/cv_20.jpg?raw=true\" alt=\"drawing\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Детекция объектов позволила бы найти 6 людей и присвоить им всем один клас - \"человек\", и нарисовать вокруг каждого из них ограничивающий прямоугольник (Bounding box). \n",
    "\n",
    "Семантическая сегментация идёт глубже и создаёт общую маску для людей, класс всё ещё остаётся единым для всех - \"человек\". \n",
    "\n",
    "Instance Segmentation - это более сложный вариант, где для каждого человека будет своя собственная маска и свой собственный класс. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Где используется семантическая сегментация? \n",
    "\n",
    "Семантическая сегментация используется в тех областях, где требуется глубокое понимание изображения. Например: \n",
    "\n",
    "* диагностика заболеваний путём сегментации клеток и тканей (медицина)\n",
    "* навигация в беспилотных автомобилях \n",
    "* разделение передного и заднего планов при редактировании фото и видео (Photoshop, и т.п.)\n",
    "* разработка роботов, которые могут перемещаться и взаимодействовать с объектами в окружающей среде "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание подходов семантической сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из подходов, используемых при сегментации изображений - это метод кодирования-декодирования (encoder-decoder). \n",
    "\n",
    "Кодировщик состоит из набора слоёв, которые извлекают элементы из изображения с помощью фильтров. \n",
    "Во многих случаях кодировщик предварительно обучается такой задаче, как классификация изображений, где он изучает корреляции из нескольких изображений. Эти знания затем могут быть переданы в процессе сегментации изображения. \n",
    "\n",
    "Окончательный результат, представляющий собой маску сегментации, генерируется декодером. \n",
    "Декодер состоит из слоёв, отвечающих за генерацию окончательного вывода. Декодер также гарантирует, что сгенерированная маска имеет сходство с разрешением в пикселях входного изображения.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/cv_21.jpg?raw=true\" alt=\"drawing\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как размечать данные для семантической сегментации? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обучить модель сегментации, нужно разметить данные. Разметка данных - это всегда довольно муторный и трудозатратный процесс, так как чем больше данных увидит модель, тем лучше она обучится.\n",
    "\n",
    "Автоматическая разметка - это прям отдельный класс задач, которые всё ещё не решены. Поэтому наиболее надёжным способом разметки данных всё ещё является старая добрая ручная разметка. \n",
    "\n",
    "Размечать можно как угодно, хоть в Paint, главное придерживаться формата, например, для сегментации, разметка должна быть в виде истинной маски. \n",
    "\n",
    "К счастью есть готовые сервисы/инструменты, которые помогают размечать данные более удобно, нежели, чем в Paint: \n",
    "\n",
    "* [Labelbox](https://labelbox.com/blog/introducing-image-segmentation/)\n",
    "* [Supervisely](https://supervise.ly/)\n",
    "* [Fritz AI](https://www.fritz.ai/)\n",
    "* [RectLabel](https://rectlabel.com/)\n",
    "* [Anolytics](https://www.anolytics.ai/semantic-segmentation-services/)\n",
    "* [Playment](https://www.playment.io/semantic-segmentation-tool)\n",
    "* [Appen](https://appen.com/)\n",
    "* [Scale.io](https://scale.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные датасеты "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас работа с сегментацией изображений стала проще благодаря наличию данных для сегментации изображений. \n",
    "Более того, некоторые из них уже встроены в некоторые фреймворки для глубокого обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Coco](https://cocodataset.org/#home) состоит из 330 тыс. изображений, 200 тыс. из которых уже размечены (то есть содержат маски и соотвествующие им классы). \n",
    "Изначально этот датасет включал в себя 91 класс объектов, которые вы можете увидеть в повседневной жизни - животные, машины, люди, диваны и т.д. Но в [документации](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) написано, что в релизы 2014 и 2017 годов вошли только 80 классов (что всё ещё довольно много).\n",
    "\n",
    "> Там же в документации есть ссылки на GitHub и общие инструкции для получения данных.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/coco.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PASCAL Visual Object Classes (PASCAL VOC)\n",
    "\n",
    "[PASCAL Visual Object Classes](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#data) состояит из 20 разных классов и 24640 аннотированных (размеченных) объекта.\n",
    "В общем, этот датасет состоит из 9963 изображений. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/pascal.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waymo\n",
    "\n",
    "[Этот датасет](https://github.com/waymo-research/waymo-open-dataset) содержит данные с датчиков с высоким разрешением, которые были собраны беспилотниками Waymo.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/waymo.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cityscapes Dataset\n",
    "\n",
    "[Этот датасет](https://www.cityscapes-dataset.com/dataset-overview/) состоит из 30 классов, 50 городов и 5 000 размеченных изображений. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/cityscapes.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambridge-driving Labeled Video Database — CamVid\n",
    "\n",
    "[Этот датасет](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/) представляет собой коллекцию видео с разметкой по семантическим классам. Всего состоит из 32 классов. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/camvid.jpg?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Berkeley Segmentation Dataset and Benchmark\n",
    "\n",
    "[Данный датасет](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/) состоит из 12 тыс. вручную размеченных изображений (из 1000 [Corel](https://sites.google.com/site/dctresearch/Home/content-based-image-retrieval) данных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Крутые архитектуры для задач сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net\n",
    "\n",
    "При обучении [данная сеть](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) сильно опирается на аугментацию данных, чтобы более эффективно использовать изображения. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/unet.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "Для оценки качества работы модели часто используется метрика IoU (intersection-over-union). Данная метрика измеряет перекрытие между истинной и предсказанной масками. \n",
    "\n",
    "$$ IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\n",
    "\n",
    "Соответственно, чем выше значение метрики IoU, тем лучше работает модель. \n",
    "\n",
    "Среднее значение IuO для U-Net: 92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastFCN — Fast Fully-connected network\n",
    "\n",
    "Подробное описание архитектуры модели представлено [здесь](https://github.com/wuhuikai/FastFCN). \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/serykhelena/PYGuides/blob/main/notebooks/assets/fastfcn.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "Среднее значение IuO для FastFCN: 53.13%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN\n",
    "\n",
    "Это модель является расширением архитектуры FastFCN. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки \n",
    "\n",
    "* [Исходная статья](https://cnvrg.io/semantic-segmentation/)\n",
    "* [Хабр статья - Deep Learning — как это работает? Часть 4](https://habr.com/ru/post/513444/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
